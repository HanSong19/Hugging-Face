{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPNEAn4t8k7xohNG3Mosjr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanSong19/Hugging-Face/blob/main/6.4%20BPE%20tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFKymdvCoaxh",
        "outputId": "d9b82103-a65a-42fc-9d29-b0caa04db578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.3.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q5X29Dv5QbQU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(type(tokenizer.backend_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojndoH-uvCC4",
        "outputId": "9a4e1631-f63b-45d8-b966-e0a32d3e8955"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tokenizers.Tokenizer'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBxGIOWvvO8H",
        "outputId": "7a5b5fea-7d1a-465d-8787-54414af74ad0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are u?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DYv-ffFvx2O",
        "outputId": "0a2f5077-7e9d-404e-aeff-e8f42451907f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)),\n",
              " (',', (5, 6)),\n",
              " ('how', (7, 10)),\n",
              " ('are', (11, 14)),\n",
              " ('you', (15, 18)),\n",
              " ('?', (18, 19))]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViQMD4HcyRtn",
        "outputId": "234763c8-fdfe-4e4b-c26d-f59ef2f777ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)),\n",
              " (',', (5, 6)),\n",
              " ('Ġhow', (6, 10)),\n",
              " ('Ġare', (10, 14)),\n",
              " ('Ġ', (14, 15)),\n",
              " ('Ġyou', (15, 19))]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmKbBEv3yeEO",
        "outputId": "71206cde-3c02-47bc-ca7f-9214e4632e36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('▁Hello,', (0, 6)),\n",
              " ('▁how', (7, 10)),\n",
              " ('▁are', (11, 14)),\n",
              " ('▁you', (16, 19))]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte-Pair Encoding tokenizer (BPE)"
      ],
      "metadata": {
        "id": "VJOY97Vo83ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ],
      "metadata": {
        "id": "76igul3q818y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "7Fzs-MiN89eA"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "  words_with_offsets= tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "\n",
        "  new_words = [word for word, offset in words_with_offsets]\n",
        "  for word in new_words:\n",
        "    word_freqs[word] += 1\n",
        "\n",
        "print(word_freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHxsM9Sd9D4b",
        "outputId": "365e79fb-0000-4b26-8a1e-b96c3df3f02b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "  for letter in word:\n",
        "    if letter not in alphabet:\n",
        "      alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "print(alphabet)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rpVnMoy-1vu",
        "outputId": "29a29f5b-8a9b-468c-9883-3b20183f7b62"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vveM2Wr0_7_I",
        "outputId": "b55df047-1f40-44b1-9480-b6d6e616f51d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
        "print(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsJu3VQnBS18",
        "outputId": "fa3e06c2-a02f-436f-b51b-e4b01303fc35"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This': ['T', 'h', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġthe': ['Ġ', 't', 'h', 'e'], 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'], 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'], 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'], '.': ['.'], 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'], 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'], 'Ġtokenization': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'], 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'], 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'], 'Ġtokenizer': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'], 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'], ',': [','], 'Ġyou': ['Ġ', 'y', 'o', 'u'], 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'], 'Ġbe': ['Ġ', 'b', 'e'], 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'], 'Ġto': ['Ġ', 't', 'o'], 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'Ġhow': ['Ġ', 'h', 'o', 'w'], 'Ġthey': ['Ġ', 't', 'h', 'e', 'y'], 'Ġare': ['Ġ', 'a', 'r', 'e'], 'Ġtrained': ['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd'], 'Ġand': ['Ġ', 'a', 'n', 'd'], 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'Ġtokens': ['Ġ', 't', 'o', 'k', 'e', 'n', 's']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_freqs.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw9rmmSPBh61",
        "outputId": "d78a2bdd-6d0f-4c70-f44e-111c5f5b6e51"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('This', 3), ('Ġis', 2), ('Ġthe', 1), ('ĠHugging', 1), ('ĠFace', 1), ('ĠCourse', 1), ('.', 4), ('Ġchapter', 1), ('Ġabout', 1), ('Ġtokenization', 1), ('Ġsection', 1), ('Ġshows', 1), ('Ġseveral', 1), ('Ġtokenizer', 1), ('Ġalgorithms', 1), ('Hopefully', 1), (',', 1), ('Ġyou', 1), ('Ġwill', 1), ('Ġbe', 1), ('Ġable', 1), ('Ġto', 1), ('Ġunderstand', 1), ('Ġhow', 1), ('Ġthey', 1), ('Ġare', 1), ('Ġtrained', 1), ('Ġand', 1), ('Ġgenerate', 1), ('Ġtokens', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pair_freqs(splits):\n",
        "  pair_freqs = defaultdict(int)\n",
        "  for w, f in word_freqs.items():\n",
        "    split=splits[w]\n",
        "    if len(split) == 1:\n",
        "      continue\n",
        "    for i in range(len(split)-1):\n",
        "      pair = (split[i], split[i+1])\n",
        "      pair_freqs[pair] += f\n",
        "  return pair_freqs\n",
        "\n"
      ],
      "metadata": {
        "id": "Ssc3gIOxCAvL"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_freqs = compute_pair_freqs(splits)\n",
        "print(pair_freqs.keys())\n",
        "print(pair_freqs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8E4Z2CJFs1T",
        "outputId": "a4278ca1-7b9a-4f23-d2d5-183ba0b95e13"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([('T', 'h'), ('h', 'i'), ('i', 's'), ('Ġ', 'i'), ('Ġ', 't'), ('t', 'h'), ('h', 'e'), ('Ġ', 'H'), ('H', 'u'), ('u', 'g'), ('g', 'g'), ('g', 'i'), ('i', 'n'), ('n', 'g'), ('Ġ', 'F'), ('F', 'a'), ('a', 'c'), ('c', 'e'), ('Ġ', 'C'), ('C', 'o'), ('o', 'u'), ('u', 'r'), ('r', 's'), ('s', 'e'), ('Ġ', 'c'), ('c', 'h'), ('h', 'a'), ('a', 'p'), ('p', 't'), ('t', 'e'), ('e', 'r'), ('Ġ', 'a'), ('a', 'b'), ('b', 'o'), ('u', 't'), ('t', 'o'), ('o', 'k'), ('k', 'e'), ('e', 'n'), ('n', 'i'), ('i', 'z'), ('z', 'a'), ('a', 't'), ('t', 'i'), ('i', 'o'), ('o', 'n'), ('Ġ', 's'), ('e', 'c'), ('c', 't'), ('s', 'h'), ('h', 'o'), ('o', 'w'), ('w', 's'), ('e', 'v'), ('v', 'e'), ('r', 'a'), ('a', 'l'), ('z', 'e'), ('l', 'g'), ('g', 'o'), ('o', 'r'), ('r', 'i'), ('i', 't'), ('h', 'm'), ('m', 's'), ('H', 'o'), ('o', 'p'), ('p', 'e'), ('e', 'f'), ('f', 'u'), ('u', 'l'), ('l', 'l'), ('l', 'y'), ('Ġ', 'y'), ('y', 'o'), ('Ġ', 'w'), ('w', 'i'), ('i', 'l'), ('Ġ', 'b'), ('b', 'e'), ('b', 'l'), ('l', 'e'), ('Ġ', 'u'), ('u', 'n'), ('n', 'd'), ('d', 'e'), ('s', 't'), ('t', 'a'), ('a', 'n'), ('Ġ', 'h'), ('e', 'y'), ('a', 'r'), ('r', 'e'), ('t', 'r'), ('a', 'i'), ('n', 'e'), ('e', 'd'), ('Ġ', 'g'), ('g', 'e'), ('n', 's')])\n",
            "defaultdict(<class 'int'>, {('T', 'h'): 3, ('h', 'i'): 3, ('i', 's'): 5, ('Ġ', 'i'): 2, ('Ġ', 't'): 7, ('t', 'h'): 3, ('h', 'e'): 2, ('Ġ', 'H'): 1, ('H', 'u'): 1, ('u', 'g'): 1, ('g', 'g'): 1, ('g', 'i'): 1, ('i', 'n'): 2, ('n', 'g'): 1, ('Ġ', 'F'): 1, ('F', 'a'): 1, ('a', 'c'): 1, ('c', 'e'): 1, ('Ġ', 'C'): 1, ('C', 'o'): 1, ('o', 'u'): 3, ('u', 'r'): 1, ('r', 's'): 2, ('s', 'e'): 3, ('Ġ', 'c'): 1, ('c', 'h'): 1, ('h', 'a'): 1, ('a', 'p'): 1, ('p', 't'): 1, ('t', 'e'): 2, ('e', 'r'): 5, ('Ġ', 'a'): 5, ('a', 'b'): 2, ('b', 'o'): 1, ('u', 't'): 1, ('t', 'o'): 4, ('o', 'k'): 3, ('k', 'e'): 3, ('e', 'n'): 4, ('n', 'i'): 2, ('i', 'z'): 2, ('z', 'a'): 1, ('a', 't'): 2, ('t', 'i'): 2, ('i', 'o'): 2, ('o', 'n'): 2, ('Ġ', 's'): 3, ('e', 'c'): 1, ('c', 't'): 1, ('s', 'h'): 1, ('h', 'o'): 2, ('o', 'w'): 2, ('w', 's'): 1, ('e', 'v'): 1, ('v', 'e'): 1, ('r', 'a'): 3, ('a', 'l'): 2, ('z', 'e'): 1, ('l', 'g'): 1, ('g', 'o'): 1, ('o', 'r'): 1, ('r', 'i'): 1, ('i', 't'): 1, ('h', 'm'): 1, ('m', 's'): 1, ('H', 'o'): 1, ('o', 'p'): 1, ('p', 'e'): 1, ('e', 'f'): 1, ('f', 'u'): 1, ('u', 'l'): 1, ('l', 'l'): 2, ('l', 'y'): 1, ('Ġ', 'y'): 1, ('y', 'o'): 1, ('Ġ', 'w'): 1, ('w', 'i'): 1, ('i', 'l'): 1, ('Ġ', 'b'): 1, ('b', 'e'): 1, ('b', 'l'): 1, ('l', 'e'): 1, ('Ġ', 'u'): 1, ('u', 'n'): 1, ('n', 'd'): 3, ('d', 'e'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'n'): 2, ('Ġ', 'h'): 1, ('e', 'y'): 1, ('a', 'r'): 1, ('r', 'e'): 1, ('t', 'r'): 1, ('a', 'i'): 1, ('n', 'e'): 2, ('e', 'd'): 1, ('Ġ', 'g'): 1, ('g', 'e'): 1, ('n', 's'): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "  print(f\"{key}: {pair_freqs[key]}\")\n",
        "  if i>=5:\n",
        "    break"
      ],
      "metadata": {
        "id": "gSYDQhuYRWPA",
        "outputId": "66e8ccb7-9875-4d87-fc4e-ebbfd9d52994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', 'h'): 3\n",
            "('h', 'i'): 3\n",
            "('i', 's'): 5\n",
            "('Ġ', 'i'): 2\n",
            "('Ġ', 't'): 7\n",
            "('t', 'h'): 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pair_freqs)"
      ],
      "metadata": {
        "id": "gagIlWcMSXyQ",
        "outputId": "078a1f45-4412-40d4-8461-c5e485687eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {('T', 'h'): 3, ('h', 'i'): 3, ('i', 's'): 5, ('Ġ', 'i'): 2, ('Ġ', 't'): 7, ('t', 'h'): 3, ('h', 'e'): 2, ('Ġ', 'H'): 1, ('H', 'u'): 1, ('u', 'g'): 1, ('g', 'g'): 1, ('g', 'i'): 1, ('i', 'n'): 2, ('n', 'g'): 1, ('Ġ', 'F'): 1, ('F', 'a'): 1, ('a', 'c'): 1, ('c', 'e'): 1, ('Ġ', 'C'): 1, ('C', 'o'): 1, ('o', 'u'): 3, ('u', 'r'): 1, ('r', 's'): 2, ('s', 'e'): 3, ('Ġ', 'c'): 1, ('c', 'h'): 1, ('h', 'a'): 1, ('a', 'p'): 1, ('p', 't'): 1, ('t', 'e'): 2, ('e', 'r'): 5, ('Ġ', 'a'): 5, ('a', 'b'): 2, ('b', 'o'): 1, ('u', 't'): 1, ('t', 'o'): 4, ('o', 'k'): 3, ('k', 'e'): 3, ('e', 'n'): 4, ('n', 'i'): 2, ('i', 'z'): 2, ('z', 'a'): 1, ('a', 't'): 2, ('t', 'i'): 2, ('i', 'o'): 2, ('o', 'n'): 2, ('Ġ', 's'): 3, ('e', 'c'): 1, ('c', 't'): 1, ('s', 'h'): 1, ('h', 'o'): 2, ('o', 'w'): 2, ('w', 's'): 1, ('e', 'v'): 1, ('v', 'e'): 1, ('r', 'a'): 3, ('a', 'l'): 2, ('z', 'e'): 1, ('l', 'g'): 1, ('g', 'o'): 1, ('o', 'r'): 1, ('r', 'i'): 1, ('i', 't'): 1, ('h', 'm'): 1, ('m', 's'): 1, ('H', 'o'): 1, ('o', 'p'): 1, ('p', 'e'): 1, ('e', 'f'): 1, ('f', 'u'): 1, ('u', 'l'): 1, ('l', 'l'): 2, ('l', 'y'): 1, ('Ġ', 'y'): 1, ('y', 'o'): 1, ('Ġ', 'w'): 1, ('w', 'i'): 1, ('i', 'l'): 1, ('Ġ', 'b'): 1, ('b', 'e'): 1, ('b', 'l'): 1, ('l', 'e'): 1, ('Ġ', 'u'): 1, ('u', 'n'): 1, ('n', 'd'): 3, ('d', 'e'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'n'): 2, ('Ġ', 'h'): 1, ('e', 'y'): 1, ('a', 'r'): 1, ('r', 'e'): 1, ('t', 'r'): 1, ('a', 'i'): 1, ('n', 'e'): 2, ('e', 'd'): 1, ('Ġ', 'g'): 1, ('g', 'e'): 1, ('n', 's'): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for p, f in pair_freqs.items():\n",
        "  if max_freq is None or max_freq <f:\n",
        "    best_pair = p\n",
        "    max_freq = f\n",
        "\n",
        "print(best_pair, max_freq)"
      ],
      "metadata": {
        "id": "nDGMX_2nTuvz",
        "outputId": "30f87081-d276-41db-a815-e0811783cb79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Ġ', 't') 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merges = {('Ġ', 't'):\"Ġt\" }\n",
        "vocab.append(\"Ġt\")"
      ],
      "metadata": {
        "id": "2Tf7HHFZUDcg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_freqs:\n",
        "  split = splits[word]\n",
        "  print(split)\n",
        "\n"
      ],
      "metadata": {
        "id": "R4m-cxvdW5It",
        "outputId": "384a8deb-da96-4198-f180-a72d89921cd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', 'h', 'i', 's']\n",
            "['Ġ', 'i', 's']\n",
            "['Ġ', 't', 'h', 'e']\n",
            "['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g']\n",
            "['Ġ', 'F', 'a', 'c', 'e']\n",
            "['Ġ', 'C', 'o', 'u', 'r', 's', 'e']\n",
            "['.']\n",
            "['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r']\n",
            "['Ġ', 'a', 'b', 'o', 'u', 't']\n",
            "['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n']\n",
            "['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n']\n",
            "['Ġ', 's', 'h', 'o', 'w', 's']\n",
            "['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l']\n",
            "['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r']\n",
            "['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's']\n",
            "['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y']\n",
            "[',']\n",
            "['Ġ', 'y', 'o', 'u']\n",
            "['Ġ', 'w', 'i', 'l', 'l']\n",
            "['Ġ', 'b', 'e']\n",
            "['Ġ', 'a', 'b', 'l', 'e']\n",
            "['Ġ', 't', 'o']\n",
            "['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd']\n",
            "['Ġ', 'h', 'o', 'w']\n",
            "['Ġ', 't', 'h', 'e', 'y']\n",
            "['Ġ', 'a', 'r', 'e']\n",
            "['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd']\n",
            "['Ġ', 'a', 'n', 'd']\n",
            "['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e']\n",
            "['Ġ', 't', 'o', 'k', 'e', 'n', 's']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "CdNzfOwbUWUS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
        "print(splits[\"Ġtrained\"] )"
      ],
      "metadata": {
        "id": "temp2YpGYOUh",
        "outputId": "d0def837-f328-41be-e1d6-a66038480959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])"
      ],
      "metadata": {
        "id": "R8FT5n07Y2Ly"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merges)\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "-i67k-cMb70v",
        "outputId": "a92a54df-44da-457a-81fe-ff1d767b676d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('Ġ', 't'): 'Ġt', ('n', 's'): 'ns', ('e', 'ns'): 'ens', ('k', 'ens'): 'kens', ('o', 'kens'): 'okens', ('Ġt', 'okens'): 'Ġtokens', ('g', 'e'): 'ge', ('ge', 'n'): 'gen', ('gen', 'e'): 'gene', ('gene', 'r'): 'gener', ('gener', 'a'): 'genera', ('genera', 't'): 'generat', ('generat', 'e'): 'generate', ('Ġ', 'generate'): 'Ġgenerate', ('e', 'd'): 'ed', ('n', 'ed'): 'ned', ('i', 'ned'): 'ined', ('a', 'ined'): 'ained', ('r', 'ained'): 'rained', ('Ġt', 'rained'): 'Ġtrained', ('r', 'e'): 're', ('a', 're'): 'are', ('Ġ', 'are'): 'Ġare', ('e', 'y'): 'ey', ('h', 'ey'): 'hey', ('Ġt', 'hey'): 'Ġthey', ('Ġ', 'h'): 'Ġh', ('Ġh', 'o'): 'Ġho', ('Ġho', 'w'): 'Ġhow', ('a', 'n'): 'an', ('Ġ', 'an'): 'Ġan', ('Ġan', 'd'): 'Ġand', ('an', 'd'): 'and', ('t', 'and'): 'tand', ('s', 'tand'): 'stand', ('r', 'stand'): 'rstand', ('e', 'rstand'): 'erstand', ('d', 'erstand'): 'derstand', ('n', 'derstand'): 'nderstand', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}\n",
            "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  pre_tokenize_results = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "  splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "  for pair, merge in merges.items():\n",
        "    for idx, split in enumerate(splits):\n",
        "      i=0\n",
        "      while i < len(split) - 1:\n",
        "        if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "            split = split[:i] + [merge] + split[i + 2 :]\n",
        "        else:\n",
        "            i += 1\n",
        "      splits[idx] = split\n",
        "  return sum(splits, [])\n",
        "\n"
      ],
      "metadata": {
        "id": "w0e-dHAfda2f"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"This is not a token\")"
      ],
      "metadata": {
        "id": "y3_KkegeeouT",
        "outputId": "3dba9bda-f06c-435d-ff85-df8219781c58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "id": "EKrY3MvwgiTk",
        "outputId": "ccd70ee6-fe47-4fef-fc10-b9883d0ea2c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'ns', 'ens', 'kens', 'okens', 'Ġtokens', 'ge', 'gen', 'gene', 'gener', 'genera', 'generat', 'generate', 'Ġgenerate', 'ed', 'ned', 'ined', 'ained', 'rained', 'Ġtrained', 're', 'are', 'Ġare', 'ey', 'hey', 'Ġthey', 'Ġh', 'Ġho', 'Ġhow', 'an', 'Ġan', 'Ġand', 'and', 'tand', 'stand', 'rstand', 'erstand', 'derstand', 'nderstand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vYEcULATqf46"
      }
    }
  ]
}