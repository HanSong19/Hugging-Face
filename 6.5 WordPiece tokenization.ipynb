{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA0c9WTHoW8CDnvNtVvNFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanSong19/Hugging-Face/blob/main/6.5%20WordPiece%20tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9hFNArhrkTQ",
        "outputId": "860c09f9-aae6-43d1-d417-1c7c2b6aaea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/519.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m512.0/519.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece])\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 huggingface-hub-0.16.4 multiprocess-0.70.15 responses-0.18.0 safetensors-0.3.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ],
      "metadata": {
        "id": "cWjjxHSJroLK"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "zZF6gvFD2o_3"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "  word_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  new_word = [word for word, offset in word_with_offsets]\n",
        "  for word in new_word:\n",
        "    word_freqs[word] += 1\n"
      ],
      "metadata": {
        "id": "ZYsTLtt_3Njc"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "\n",
        "    word_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    print(\"this is word_with offsets:\", word_with_offsets)\n",
        "    new_word = [word for word, offset in word_with_offsets]\n",
        "    print(\"this is new word:\", new_word)\n",
        "    for word in new_word:\n",
        "      word_freqs[word] += 1\n",
        "      print(\"this is word_freq:\",word_freqs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHwCZ7FU3k20",
        "outputId": "7f7312b9-5245-4901-c6b8-8b757d0f37e0"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is word_with offsets: [('This', (0, 4)), ('is', (5, 7)), ('the', (8, 11)), ('Hugging', (12, 19)), ('Face', (20, 24)), ('Course', (25, 31)), ('.', (31, 32))]\n",
            "this is new word: ['This', 'is', 'the', 'Hugging', 'Face', 'Course', '.']\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 1, 'is': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 1, 'is': 1, 'the': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 1, 'is': 1, 'the': 1, 'Hugging': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 1, 'is': 1, 'the': 1, 'Hugging': 1, 'Face': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 1, 'is': 1, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 1, 'is': 1, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 1})\n",
            "this is word_with offsets: [('This', (0, 4)), ('chapter', (5, 12)), ('is', (13, 15)), ('about', (16, 21)), ('tokenization', (22, 34)), ('.', (34, 35))]\n",
            "this is new word: ['This', 'chapter', 'is', 'about', 'tokenization', '.']\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 2, 'is': 1, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 2, 'is': 1, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 1, 'chapter': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 2, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 1, 'chapter': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 2, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 1, 'chapter': 1, 'about': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 2, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 1, 'chapter': 1, 'about': 1, 'tokenization': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 2, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 2, 'chapter': 1, 'about': 1, 'tokenization': 1})\n",
            "this is word_with offsets: [('This', (0, 4)), ('section', (5, 12)), ('shows', (13, 18)), ('several', (19, 26)), ('tokenizer', (27, 36)), ('algorithms', (37, 47)), ('.', (47, 48))]\n",
            "this is new word: ['This', 'section', 'shows', 'several', 'tokenizer', 'algorithms', '.']\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 2, 'chapter': 1, 'about': 1, 'tokenization': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 2, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 2, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 2, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 2, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 2, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1})\n",
            "this is word_with offsets: [('Hopefully', (0, 9)), (',', (9, 10)), ('you', (11, 14)), ('will', (15, 19)), ('be', (20, 22)), ('able', (23, 27)), ('to', (28, 30)), ('understand', (31, 41)), ('how', (42, 45)), ('they', (46, 50)), ('are', (51, 54)), ('trained', (55, 62)), ('and', (63, 66)), ('generate', (67, 75)), ('tokens', (76, 82)), ('.', (82, 83))]\n",
            "this is new word: ['Hopefully', ',', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens', '.']\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 3, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})\n",
            "this is word_freq: defaultdict(<class 'int'>, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atqpZI_u48U0",
        "outputId": "77cb132b-e02e-420f-cbce-6474c04dcd55"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'This': 3,\n",
              "             'is': 2,\n",
              "             'the': 1,\n",
              "             'Hugging': 1,\n",
              "             'Face': 1,\n",
              "             'Course': 1,\n",
              "             '.': 4,\n",
              "             'chapter': 1,\n",
              "             'about': 1,\n",
              "             'tokenization': 1,\n",
              "             'section': 1,\n",
              "             'shows': 1,\n",
              "             'several': 1,\n",
              "             'tokenizer': 1,\n",
              "             'algorithms': 1,\n",
              "             'Hopefully': 1,\n",
              "             ',': 1,\n",
              "             'you': 1,\n",
              "             'will': 1,\n",
              "             'be': 1,\n",
              "             'able': 1,\n",
              "             'to': 1,\n",
              "             'understand': 1,\n",
              "             'how': 1,\n",
              "             'they': 1,\n",
              "             'are': 1,\n",
              "             'trained': 1,\n",
              "             'and': 1,\n",
              "             'generate': 1,\n",
              "             'tokens': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "  if word[0] not in alphabet:\n",
        "    alphabet.append(word[0])\n",
        "  for letter in word[1:]:\n",
        "    if f\"##{letter}\" not in alphabet:\n",
        "      alphabet.append(f\"##{letter}\")\n",
        "alphabet.sort()\n",
        "\n",
        "alphabet\n",
        "\n",
        "print(alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psLmzq6U4-if",
        "outputId": "60ddcc43-5204-414a-ec82-2b947307eff8"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCp0O_t26El3",
        "outputId": "25eca867-93d0-436e-8a17-2d6b5c1b7d60"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {\n",
        "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}\n",
        "\n",
        "print(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0dzWBVt-HhT",
        "outputId": "df59283d-c720-46bf-88f7-ce4bd19504b5"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This': ['T', '##h', '##i', '##s'], 'is': ['i', '##s'], 'the': ['t', '##h', '##e'], 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'], 'Face': ['F', '##a', '##c', '##e'], 'Course': ['C', '##o', '##u', '##r', '##s', '##e'], '.': ['.'], 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'], 'about': ['a', '##b', '##o', '##u', '##t'], 'tokenization': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'], 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'], 'shows': ['s', '##h', '##o', '##w', '##s'], 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'], 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'algorithms': ['a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'], 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'], ',': [','], 'you': ['y', '##o', '##u'], 'will': ['w', '##i', '##l', '##l'], 'be': ['b', '##e'], 'able': ['a', '##b', '##l', '##e'], 'to': ['t', '##o'], 'understand': ['u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'], 'how': ['h', '##o', '##w'], 'they': ['t', '##h', '##e', '##y'], 'are': ['a', '##r', '##e'], 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'], 'and': ['a', '##n', '##d'], 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pair_scores(splits):\n",
        "  letter_freqs = defaultdict(int)\n",
        "  pair_freqs = defaultdict(int)\n",
        "  for word, freq in word_freqs.items():\n",
        "    split = splits[word]\n",
        "    if len(split) ==1:\n",
        "      letter_freqs[split[0]] += freq\n",
        "      continue\n",
        "    for i in range(len(split)-1):\n",
        "      pair = (split[i], split[i+1])\n",
        "      letter_freqs[split[i]] += freq\n",
        "      pair_freqs[pair] += freq\n",
        "    letter_freqs[split[-1]] += freq\n",
        "  scores = {\n",
        "      pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "      for pair, freq in pair_freqs.items()\n",
        "  }\n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "hTfau6ONDLZx"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_scores= compute_pair_scores(splits)\n",
        "\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "  print(f\"{key} : {pair_scores[key]}\")\n",
        "  if i>=5:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-0U0V17HrjH",
        "outputId": "66220493-e1d8-4275-b980-64eb55c9383e"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', '##h') : 0.125\n",
            "('##h', '##i') : 0.03409090909090909\n",
            "('##i', '##s') : 0.02727272727272727\n",
            "('i', '##s') : 0.1\n",
            "('t', '##h') : 0.03571428571428571\n",
            "('##h', '##e') : 0.011904761904761904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pair_scores.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uEfBoplISou",
        "outputId": "4b7b5bf7-6800-4190-aabb-d38265287c80"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([(('T', '##h'), 0.125), (('##h', '##i'), 0.03409090909090909), (('##i', '##s'), 0.02727272727272727), (('i', '##s'), 0.1), (('t', '##h'), 0.03571428571428571), (('##h', '##e'), 0.011904761904761904), (('H', '##u'), 0.1), (('##u', '##g'), 0.05), (('##g', '##g'), 0.0625), (('##g', '##i'), 0.022727272727272728), (('##i', '##n'), 0.01652892561983471), (('##n', '##g'), 0.022727272727272728), (('F', '##a'), 0.14285714285714285), (('##a', '##c'), 0.07142857142857142), (('##c', '##e'), 0.023809523809523808), (('C', '##o'), 0.07692307692307693), (('##o', '##u'), 0.046153846153846156), (('##u', '##r'), 0.022222222222222223), (('##r', '##s'), 0.022222222222222223), (('##s', '##e'), 0.004761904761904762), (('c', '##h'), 0.125), (('##h', '##a'), 0.017857142857142856), (('##a', '##p'), 0.07142857142857142), (('##p', '##t'), 0.07142857142857142), (('##t', '##e'), 0.013605442176870748), (('##e', '##r'), 0.026455026455026454), (('a', '##b'), 0.2), (('##b', '##o'), 0.038461538461538464), (('##u', '##t'), 0.02857142857142857), (('t', '##o'), 0.04395604395604396), (('##o', '##k'), 0.07692307692307693), (('##k', '##e'), 0.047619047619047616), (('##e', '##n'), 0.017316017316017316), (('##n', '##i'), 0.01652892561983471), (('##i', '##z'), 0.09090909090909091), (('##z', '##a'), 0.07142857142857142), (('##a', '##t'), 0.04081632653061224), (('##t', '##i'), 0.025974025974025976), (('##i', '##o'), 0.013986013986013986), (('##o', '##n'), 0.013986013986013986), (('s', '##e'), 0.031746031746031744), (('##e', '##c'), 0.023809523809523808), (('##c', '##t'), 0.07142857142857142), (('s', '##h'), 0.041666666666666664), (('##h', '##o'), 0.009615384615384616), (('##o', '##w'), 0.07692307692307693), (('##w', '##s'), 0.05), (('##e', '##v'), 0.047619047619047616), (('##v', '##e'), 0.047619047619047616), (('##r', '##a'), 0.047619047619047616), (('##a', '##l'), 0.02040816326530612), (('##z', '##e'), 0.023809523809523808), (('a', '##l'), 0.02857142857142857), (('##l', '##g'), 0.03571428571428571), (('##g', '##o'), 0.019230769230769232), (('##o', '##r'), 0.008547008547008548), (('##r', '##i'), 0.010101010101010102), (('##i', '##t'), 0.012987012987012988), (('##t', '##h'), 0.017857142857142856), (('##h', '##m'), 0.125), (('##m', '##s'), 0.1), (('H', '##o'), 0.038461538461538464), (('##o', '##p'), 0.038461538461538464), (('##p', '##e'), 0.023809523809523808), (('##e', '##f'), 0.047619047619047616), (('##f', '##u'), 0.2), (('##u', '##l'), 0.02857142857142857), (('##l', '##l'), 0.04081632653061224), (('##l', '##y'), 0.07142857142857142), (('y', '##o'), 0.07692307692307693), (('w', '##i'), 0.09090909090909091), (('##i', '##l'), 0.012987012987012988), (('b', '##e'), 0.047619047619047616), (('##b', '##l'), 0.07142857142857142), (('##l', '##e'), 0.006802721088435374), (('u', '##n'), 0.09090909090909091), (('##n', '##d'), 0.06818181818181818), (('##d', '##e'), 0.011904761904761904), (('##s', '##t'), 0.014285714285714285), (('##t', '##a'), 0.02040816326530612), (('##a', '##n'), 0.012987012987012988), (('h', '##o'), 0.07692307692307693), (('##e', '##y'), 0.023809523809523808), (('a', '##r'), 0.022222222222222223), (('##r', '##e'), 0.005291005291005291), (('t', '##r'), 0.015873015873015872), (('##a', '##i'), 0.012987012987012988), (('##n', '##e'), 0.008658008658008658), (('##e', '##d'), 0.011904761904761904), (('a', '##n'), 0.01818181818181818), (('g', '##e'), 0.047619047619047616), (('##n', '##s'), 0.00909090909090909)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_pair = \"\"\n",
        "max_score = None\n",
        "\n",
        "for pair, score in pair_scores.items():\n",
        "  if max_score is None or max_score < score:\n",
        "    best_pair=pair\n",
        "    max_score=score\n",
        "\n",
        "print(best_pair, max_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikdrzqHwINS1",
        "outputId": "6a393588-2d36-459e-98b9-47f7c3081b00"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', '##b') 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair (a,b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) ==1:\n",
        "            continue\n",
        "        i=0\n",
        "        while i < len(split) -1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "               merge = a+b[2:] if b.startswith(\"##\") else a+b\n",
        "               split = split[:i] + [merge] + split [i+2:]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "JNhzFtNnIj9p"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = merge_pair(\"a\", \"##b\", splits)\n",
        "splits[\"about\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSlXk7PKZ4kd",
        "outputId": "d2fa291a-51d3-4563-f6d5-c3da07501eaa"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ab', '##o', '##u', '##t']"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=70\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "  scores = compute_pair_scores(splits)\n",
        "  best_pair, max_score = \"\", None\n",
        "  for pair, score in scores.items():\n",
        "    if max_score is None or max_score <score:\n",
        "      best_pair = pair\n",
        "      max_score = score\n",
        "  splits = merge_pair(*best_pair, splits)\n",
        "  new_token = (\n",
        "      best_pair[0] + best_pair[1][2:]\n",
        "      if best_pair[1].startswith(\"##\")\n",
        "      else best_pair[0] + best_pair[1]\n",
        "  )\n",
        "  vocab.append(new_token)\n",
        "\n"
      ],
      "metadata": {
        "id": "N1vhsIuecTyW"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0AspWA5fI4U",
        "outputId": "252e64fa-4798-4131-fc5c-a6b823220b2d"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut', '##ta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_word(word):\n",
        "  tokens=[]\n",
        "  while len(word) > 0:\n",
        "    i = len(word)\n",
        "    while i >0 and word[:i] not in vocab:\n",
        "      i -= 1\n",
        "    if i ==0:\n",
        "      return [\"[UNK]\"]\n",
        "    tokens.append(word[:i])\n",
        "    word = word[i:]\n",
        "    if len(word) > 0:\n",
        "      word = f\"##{word}\"\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "VXFUsoRFfMUV"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode_word(\"Hugging\"))\n",
        "print(encode_word(\"HOgging\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRjuxhtDgnCB",
        "outputId": "bffc9470-abeb-4f04-8059-e0e9a0e54af2"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hugg', '##i', '##n', '##g']\n",
            "['[UNK]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  pre_tokenize_results = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  pre_tokenized_text = [word for word, offset in pre_tokenize_results]\n",
        "  encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "  return sum(encoded_words, [])"
      ],
      "metadata": {
        "id": "TtOHs0d2gpy0"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"Is this the Hugging Face course that I looked for\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Z0Y0EghB5y",
        "outputId": "103b39a4-5c4c-4532-cd81-105b161b9293"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]',\n",
              " 'th',\n",
              " '##i',\n",
              " '##s',\n",
              " 'th',\n",
              " '##e',\n",
              " 'Hugg',\n",
              " '##i',\n",
              " '##n',\n",
              " '##g',\n",
              " 'Fac',\n",
              " '##e',\n",
              " 'c',\n",
              " '##o',\n",
              " '##u',\n",
              " '##r',\n",
              " '##s',\n",
              " '##e',\n",
              " 'th',\n",
              " '##a',\n",
              " '##t',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " '[UNK]']"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    }
  ]
}